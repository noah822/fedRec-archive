{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea489dec-d26e-4bd1-8218-bf892ba47963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/fedRec\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "HEAD_CKP = None\n",
    "BACKBONE_CKP = './ckp/components/99/debug_mm_only_iid.pt'\n",
    "# BACKBONE_CKP = './extra/reconable_vanilla.pt'\n",
    "NUM_CLIENT = 10\n",
    "PROBE_PATH = './clients/probe'\n",
    "TEST_PATH = './clients/test'\n",
    "SEED = 272\n",
    "# 43\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "%cd /root/fedRec\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import random\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "from fed.config import STATE\n",
    "from typing import Callable, List, Any, Tuple\n",
    "    \n",
    "CLIENT_STATES = [\n",
    " STATE.AUDIO,\n",
    " STATE.BOTH,\n",
    " STATE.BOTH,\n",
    " STATE.AUDIO,\n",
    " STATE.IMAGE,\n",
    " STATE.IMAGE,\n",
    " STATE.AUDIO,\n",
    " STATE.IMAGE,\n",
    " STATE.BOTH,\n",
    " STATE.BOTH]\n",
    "\n",
    "    \n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25997d7c-3e73-450b-b30b-8e5e97fb25cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from experiments.mmvae.mnist.model import (\n",
    "    get_mnist_image_encoder,\n",
    "    get_mnist_audio_encoder\n",
    ")\n",
    "from typing import List, Tuple\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from model.simclr import StandardPipeline\n",
    "\n",
    "\n",
    "def get_proj_head():\n",
    "    return nn.Linear(64, 128)\n",
    "\n",
    "\n",
    "version = 'new'\n",
    "\n",
    "def load_models(ckp_path):\n",
    "    ckp = torch.load(ckp_path)\n",
    "    \n",
    "    if version == 'new':\n",
    "        audio_model = get_mnist_audio_encoder().to(device)\n",
    "        image_model = get_mnist_image_encoder().to(device)\n",
    "\n",
    "        audio_model.load_state_dict(ckp['audio'])\n",
    "        image_model.load_state_dict(ckp['image'])\n",
    "        \n",
    "        audio_model.eval()\n",
    "        image_model.eval()\n",
    "\n",
    "        return audio_model, image_model\n",
    "    else: # version = 'old'\n",
    "        audio_model = get_mnist_audio_encoder().to(device)\n",
    "        image_model = get_mnist_image_encoder().to(device)\n",
    "            \n",
    "\n",
    "        audio_model.load_state_dict(ckp['audio'])\n",
    "        image_model.load_state_dict(ckp['image'])\n",
    "\n",
    "        return audio_model, image_model\n",
    "        \n",
    "\n",
    "import torchvision.transforms as T\n",
    "from experiments.mmvae.mnist.dataset import imageMNIST, audioMNIST, mmMNIST\n",
    "\n",
    "from experiments.ssl.dataset import get_mnist_transform\n",
    "\n",
    "def load_dls(cid, probe_path, test_path, dl_config=None):\n",
    "    if dl_config is None:\n",
    "        dl_config = {\n",
    "            'batch_size' : 64,\n",
    "            'shuffle' : True\n",
    "        }\n",
    "    probe_dls = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    audio_probe_set = audioMNIST(csv_path=f'{probe_path}/{cid}_audio.csv')\n",
    "    \n",
    "    _, transform = get_mnist_transform()\n",
    "    transform = T.Compose(transform)\n",
    "    image_probe_set = imageMNIST(csv_path=f'{probe_path}/{cid}_image.csv', \n",
    "                                 transform=transform)\n",
    "    \n",
    "    probe_dls = [\n",
    "        DataLoader(audio_probe_set, **dl_config),\n",
    "        DataLoader(image_probe_set, **dl_config)\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    test_dls = []\n",
    "    test_dl_config = {\n",
    "        'batch_size' : 64,\n",
    "        'shuffle' : False\n",
    "    }\n",
    "    \n",
    "    audio_test_set = audioMNIST(csv_path=f'{test_path}/{cid}_audio.csv')\n",
    "    _, transform = get_mnist_transform()\n",
    "    transform = T.Compose(transform)\n",
    "    \n",
    "    image_test_set = imageMNIST(csv_path=f'{test_path}/{cid}_image.csv', transform=transform)\n",
    "    test_dls = [\n",
    "        DataLoader(audio_test_set, **test_dl_config),\n",
    "        DataLoader(image_test_set, **test_dl_config)\n",
    "    ]\n",
    "    return probe_dls, test_dls\n",
    "\n",
    "def fused_load_dls(cid, probe_path, test_path, dl_config=None):\n",
    "    if dl_config is None:\n",
    "        dl_config = {\n",
    "            'batch_size' : 64,\n",
    "            'shuffle' : True\n",
    "        }\n",
    "    \n",
    "    client_state = CLIENT_STATES[cid]\n",
    "    \n",
    "    if client_state == STATE.AUDIO:\n",
    "        probe_set = audioMNIST(csv_path=f'{probe_path}/{cid}_audio.csv')\n",
    "        test_set = audioMNIST(csv_path=f'{test_path}/{cid}_audio.csv')\n",
    "    elif client_state == STATE.IMAGE:\n",
    "        _, transform = get_mnist_transform()\n",
    "        transform = T.Compose(transform)\n",
    "        probe_set = imageMNIST(csv_path=f'{probe_path}/{cid}_image.csv', \n",
    "                                 transform=transform)\n",
    "        test_set = imageMNIST(csv_path=f'{test_path}/{cid}_image.csv', \n",
    "                                 transform=transform)\n",
    "    else:\n",
    "        _, transform = get_mnist_transform()\n",
    "        transform = T.Compose(transform)\n",
    "        probe_set = mmMNIST(csv_path=f'{probe_path}/{cid}.csv',\n",
    "                            image_transform=transform,\n",
    "                            with_label=True)\n",
    "        test_set = mmMNIST(csv_path=f'{test_path}/{cid}.csv',\n",
    "                           image_transform=transform,\n",
    "                           with_label=True)\n",
    "        \n",
    "    \n",
    "    \n",
    "    probe_dl = DataLoader(probe_set, **dl_config)\n",
    "    \n",
    "    test_dl_config = {\n",
    "        'batch_size' : 64,\n",
    "        'shuffle' : False\n",
    "    }\n",
    "    test_dl = DataLoader(test_set, **test_dl_config)\n",
    "    \n",
    "    return probe_dl, test_dl\n",
    "\n",
    "\n",
    "def pretty_print(cid, image_acc, audio_acc):\n",
    "    state = CLIENT_STATES[cid]\n",
    "    display = (\"[CLIENT {}] -> {}  \\n\"\n",
    "               \"Audio Accuracy: {:.2f}\\n\"\n",
    "               \"Image Accuracy: {:.2f}\"\n",
    "              ).format(cid, state, image_acc, audio_acc)\n",
    "    print(display)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f738bfa-5d9d-4731-b8d3-239a7e8d13a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train import val, linear_prob\n",
    "import torch.nn as nn\n",
    "from experiments.mmvae.mnist.model import _make_mlp\n",
    "\n",
    "class StandardArch(nn.Module):\n",
    "    def __init__(self,\n",
    "                 backbone,\n",
    "                 head,\n",
    "                 normalize=False):\n",
    "        super().__init__()\n",
    "        self.backbone, self.head = backbone, head\n",
    "        self._norm = normalize\n",
    "    def forward(self, x):\n",
    "        feature = self.backbone(x)\n",
    "        if self._norm:\n",
    "            feature = F.normalize(feature, dim=-1)\n",
    "        pred = self.head(feature)\n",
    "        return pred\n",
    "    \n",
    "def load_rec_net(path, key):\n",
    "    # net = _make_mlp(128, 256, 128, use_bn=False).to(device)\n",
    "    net = nn.Sequential(\n",
    "        nn.Linear(64, 128),\n",
    "        nn.LayerNorm(128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 64)\n",
    "    ).to(device)\n",
    "    ckp = torch.load(path)\n",
    "    net.load_state_dict(ckp[key])\n",
    "    return net\n",
    "    \n",
    "def fused_fwd(models, inputs,\n",
    "              put_first=False,\n",
    "              rec_net=None):\n",
    "    embeds = []\n",
    "    if rec_net is None:\n",
    "        for model, x in zip(models, inputs):\n",
    "            embeds.append(model(x))\n",
    "    else:\n",
    "        # do reconstruction\n",
    "        possessed = models(inputs)\n",
    "        generated = rec_net(possessed)\n",
    "        if put_first:\n",
    "            embeds = [possessed, generated]\n",
    "        else:\n",
    "            embeds = [generated, possessed]\n",
    "    return torch.concat(embeds, dim=-1).detach()\n",
    "\n",
    "\n",
    "probe_optim_config = {\n",
    "    'lr' : 1e-1,\n",
    "    'weight_decay' : 1e-5\n",
    "}\n",
    "\n",
    "# fusion version\n",
    "def fused_run(recon: bool=False):\n",
    "    res: List[Tuple[float, float]] = [] \n",
    "    # loop over clients\n",
    "    for cid in range(NUM_CLIENT):\n",
    "        audio_backbone, image_backbone = load_models(BACKBONE_CKP)\n",
    "        \n",
    "        audio_backbone.eval()\n",
    "        image_backbone.eval()\n",
    "        \n",
    "        backbones = [audio_backbone, image_backbone]\n",
    "\n",
    "        # prepare probing loader and test_loader\n",
    "        probe_dl, test_dl = fused_load_dls(cid, PROBE_PATH, TEST_PATH)\n",
    "\n",
    "        # if multi modal are present, use concatenate features as fused feature\n",
    "        # otherwise, use single modal feature only\n",
    "        if CLIENT_STATES[cid] == STATE.BOTH:\n",
    "            head = nn.Linear(128, 10).to(device)\n",
    "            # head = nn.Sequential(\n",
    "            #     nn.Linear(128, 512),\n",
    "            #     nn.ReLU(),\n",
    "            #     nn.Linear(512, 10)\n",
    "            # ).to(device)\n",
    "            optimizer = optim.Adam(head.parameters(), lr=1e-1, weight_decay=1e-5)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            \n",
    "            def unpack_and_forward(model, inputs):\n",
    "                inputs = [t.to(device) for t in inputs]\n",
    "                audio_x, image_x, y = inputs\n",
    "                fused_embed = fused_fwd(model, [audio_x, image_x])\n",
    "                pred = head(fused_embed)\n",
    "                loss = criterion(pred, y)\n",
    "                return loss\n",
    "            extractor = backbones\n",
    "        else:\n",
    "            state = CLIENT_STATES[cid]\n",
    "            if state == STATE.AUDIO:\n",
    "                extractor = audio_backbone\n",
    "                if recon:\n",
    "                    constructor = load_rec_net(BACKBONE_CKP, 'a2i')\n",
    "            else:\n",
    "                extractor = image_backbone\n",
    "                if recon:\n",
    "                    constructor = load_rec_net(BACKBONE_CKP, 'i2a')\n",
    "            if recon:\n",
    "                extractor = [extractor, constructor]\n",
    "                # head = nn.Sequential(\n",
    "                #         nn.Linear(128, 512),\n",
    "                #         nn.ReLU(),\n",
    "                #         nn.Linear(512, 10)\n",
    "                #     ).to(device)\n",
    "                head = nn.Linear(128, 10).to(device)\n",
    "                optimizer = optim.Adam(head.parameters(), lr=1e-1, weight_decay=1e-5)\n",
    "            else:\n",
    "                # head = nn.Sequential(\n",
    "                #         nn.Linear(64, 512),\n",
    "                #         nn.ReLU(),\n",
    "                #         nn.Linear(512, 10)\n",
    "                #     ).to(device)\n",
    "                head = nn.Linear(64, 10).to(device)\n",
    "                optimizer = optim.Adam(head.parameters(), lr=1e-1, weight_decay=1e-5)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            \n",
    "            if recon:\n",
    "                def rec_then_forward(model, inputs):\n",
    "                    inputs = [t.to(device) for t in inputs]\n",
    "                    encoder, constructor = model\n",
    "                    x, y = inputs\n",
    "                    put_first = (state == STATE.AUDIO)\n",
    "                    fused_embed = fused_fwd(encoder, x, put_first, constructor)\n",
    "                    pred = head(fused_embed)\n",
    "                    loss = criterion(pred, y)\n",
    "                    return loss\n",
    "                unpack_and_forward = rec_then_forward\n",
    "            else:\n",
    "                unpack_and_forward = None\n",
    "\n",
    "        # probe the head\n",
    "        linear_prob(\n",
    "            extractor, head,\n",
    "            probe_dl,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            device,\n",
    "            35,\n",
    "            use_tqdm=False,\n",
    "            normalize=False,\n",
    "            unpack_and_forward=unpack_and_forward\n",
    "        )\n",
    "\n",
    "        # eval on the test set\n",
    "        if CLIENT_STATES[cid] == STATE.BOTH:\n",
    "            test_model = [*extractor, head]\n",
    "            def _test_unpack_and_fwd(model, inputs):\n",
    "                head = model[-1]\n",
    "                inputs = [t.to(device) for t in inputs]\n",
    "                fused_feature = fused_fwd(model[:-1], inputs[:-1])\n",
    "                pred = head(fused_feature)\n",
    "                return pred\n",
    "        else:\n",
    "            if recon:\n",
    "                test_model = [*extractor, head]\n",
    "                def _test_unpack_and_fwd(model, inputs):\n",
    "                    encoder, constructor, head = model\n",
    "                    inputs = [t.to(device) for t in inputs]\n",
    "                    x, y = inputs\n",
    "                    put_first = (CLIENT_STATES[cid] == STATE.AUDIO)\n",
    "                    fused_feature = fused_fwd(encoder, x, put_first, constructor)\n",
    "                    pred = head(fused_feature)\n",
    "                    return pred\n",
    "            else:\n",
    "                test_model = nn.Sequential(\n",
    "                    extractor, head\n",
    "                ).to(device)\n",
    "                _test_unpack_and_fwd = None\n",
    "\n",
    "        acc = val(test_model, test_dl, device, _test_unpack_and_fwd)\n",
    "\n",
    "        \n",
    "        res.append(acc)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e93dc21-ecb5-498a-8657-e7984cd85348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train import val, linear_prob\n",
    "import torch.nn as nn\n",
    "\n",
    "class StandardArch(nn.Module):\n",
    "    def __init__(self,\n",
    "                 backbone,\n",
    "                 head,\n",
    "                 normalize=False):\n",
    "        super().__init__()\n",
    "        self.backbone, self.head = backbone, head\n",
    "        self._norm = normalize\n",
    "    def forward(self, x):\n",
    "        feature = self.backbone(x)\n",
    "        if self._norm:\n",
    "            feature = F.normalize(feature, dim=-1)\n",
    "        pred = self.head(feature)\n",
    "        return pred\n",
    "        \n",
    "\n",
    "\n",
    "def run():\n",
    "    res: List[Tuple[float, float]] = [] \n",
    "    # loop over clients\n",
    "    for cid in range(NUM_CLIENT):\n",
    "        audio_backbone, image_backbone = load_models(BACKBONE_CKP)\n",
    "        backbones = [audio_backbone, image_backbone]\n",
    "\n",
    "        # prepare probing loader and test_loader\n",
    "        probe_dls, test_dls = load_dls(cid, PROBE_PATH, TEST_PATH)\n",
    "        client_res = []\n",
    "\n",
    "        # iteratively train a linear head and test it client-wise\n",
    "        # uniformly draw probing set & test set\n",
    "        for i in range(len(backbones)):\n",
    "            extractor = backbones[i]\n",
    "            head = nn.Linear(64, 10).to(device)\n",
    "            optimizer = optim.Adam(head.parameters(), lr=1e-1, weight_decay=1e-5)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "            probe_dl = probe_dls[i]\n",
    "            test_dl = test_dls[i]\n",
    "\n",
    "            # probe the head\n",
    "            linear_prob(\n",
    "                extractor, head,\n",
    "                probe_dl,\n",
    "                optimizer,\n",
    "                criterion,\n",
    "                device,\n",
    "                35,\n",
    "                use_tqdm=False,\n",
    "                normalize=False\n",
    "            )\n",
    "\n",
    "            # reconfig\n",
    "            extractor.eval()\n",
    "\n",
    "            # eval on the test set\n",
    "            model = StandardArch(extractor, head, normalize=False).to(device)\n",
    "            acc = val(model, test_dl, device)\n",
    "            client_res.append(acc)\n",
    "\n",
    "        \n",
    "        res.append(client_res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9cbcb03-163c-4208-9662-250095766de9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def duplicate_runs(fn: Callable[[Any], List[float]],\n",
    "                   times: int,\n",
    "                   aggregator: Callable=None,\n",
    "                   seeds: List[int]=None):\n",
    "    collected_res = []\n",
    "\n",
    "    # default aggreator is mean function\n",
    "    if aggregator is None:\n",
    "        def aggregator(res: List):\n",
    "            return sum(res) / len(res)\n",
    "        \n",
    "    agg_res = []\n",
    "    for idx in range(times):\n",
    "        if seeds is not None:\n",
    "            seed_everything(seeds[idx])\n",
    "        one_time_res = fn()\n",
    "        collected_res.append(one_time_res)\n",
    "\n",
    "        # transpose two-dimensionaly list\n",
    "        collected_res_t = [list(x) for x in zip(*collected_res)]\n",
    "    \n",
    "    for res_tuple in collected_res_t:\n",
    "        agg_res.append(aggregator(res_tuple))\n",
    "    \n",
    "    return agg_res\n",
    "\n",
    "def agg(t: List):\n",
    "    res = []\n",
    "    transposed_t = [list(x) for x in zip(*t)]\n",
    "    for grouped_res in transposed_t:\n",
    "        res.append(sum(grouped_res) / len(grouped_res))\n",
    "    return res\n",
    "\n",
    "# res = duplicate_runs(fused_run, 5, agg)\n",
    "import numpy as np\n",
    "multi_run = []\n",
    "for _ in range(3):\n",
    "    one_time_res = [i.cpu().numpy() for i in fused_run(recon=False)]\n",
    "    multi_run.append(one_time_res)\n",
    "res = np.array(multi_run).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a416f89-956f-4ce2-9e50-0b60e1eda644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0[STATE.AUDIO]: 0.98\n",
      "Client 1[STATE.BOTH]: 0.99\n",
      "Client 2[STATE.BOTH]: 1.00\n",
      "Client 3[STATE.AUDIO]: 0.99\n",
      "Client 4[STATE.IMAGE]: 0.97\n",
      "Client 5[STATE.IMAGE]: 0.98\n",
      "Client 6[STATE.AUDIO]: 1.00\n",
      "Client 7[STATE.IMAGE]: 0.97\n",
      "Client 8[STATE.BOTH]: 0.99\n",
      "Client 9[STATE.BOTH]: 0.99\n"
     ]
    }
   ],
   "source": [
    "# for cid, (audio_acc, image_acc) in enumerate(res):\n",
    "#     pretty_print(cid, audio_acc, image_acc)\n",
    "    \n",
    "for cid, acc in enumerate(res):\n",
    "    print('Client {}[{}]: {:.2f}'.format(cid, CLIENT_STATES[cid], acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c672a16-17a2-43cb-8da5-b34c29c99e25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import multiprocessing as mp\n",
    "# exec_fun = run\n",
    "# def wrapped_run(res_collector):\n",
    "#     res_collector.append(exec_fun())\n",
    "    \n",
    "# num_worker = 5\n",
    "# manager = mp.Manager()\n",
    "# res_container = manager.list()\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # mp.set_start_method('spawn')\n",
    "\n",
    "#     workers = []\n",
    "#     for _ in range(num_worker):\n",
    "#         p = mp.Process(target=wrapped_run, args=(res_container, ))\n",
    "#         p.start()\n",
    "#         workers.append(p)\n",
    "#     for p in workers:\n",
    "#         p.join()\n",
    "\n",
    "# def agg(t: List):\n",
    "#     res = []\n",
    "#     transposed_t = [list(x) for x in zip(*t)]\n",
    "#     for grouped_res in transposed_t:\n",
    "#         res.append(sum(grouped_res) / len(grouped_res))\n",
    "#     return res\n",
    "\n",
    "# agg_res = agg(res_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3131b5f-f4e0-47d3-b110-79e80dc3a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cid, (audio_acc, image_acc) in enumerate(res):\n",
    "#     pretty_print(cid, audio_acc, image_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
