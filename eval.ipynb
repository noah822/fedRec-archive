{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84626b9c-7cd8-4e2b-b4c5-05c7461b774b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from experiments.mmvae.mnist.model import (\n",
    "    _make_mlp,\n",
    "    get_mnist_audio_encoder,\n",
    "    get_mnist_image_encoder\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "MODAL = 'audio'\n",
    "SWITCH = 'hybrid'\n",
    "\n",
    "HEAD_CKP_PATH = './ckp/head/272/PoE_audio.pt'\n",
    "MAIN_CKP_PATH = './ckp/backbone/272/PoE.pt'\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# load pretraiend models\n",
    "head_ckp = torch.load(HEAD_CKP_PATH)\n",
    "backbone_ckp = torch.load(MAIN_CKP_PATH)\n",
    "\n",
    "head = nn.Linear(64, 10)\n",
    "head.load_state_dict(head_ckp)\n",
    "\n",
    "if MODAL == 'audio':\n",
    "    backbone = get_mnist_audio_encoder()\n",
    "else: # image\n",
    "    backbone = get_mnist_image_encoder()\n",
    "backbone.load_state_dict(backbone_ckp[MODAL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c14d22e5-c7a7-4adf-a74a-b39935e22543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 backbone,\n",
    "                 head\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.backbone, self.head = backbone, head\n",
    "        \n",
    "        # toggle feature extractor state\n",
    "        self.backbone.eval()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        feature = self.backbone(x)\n",
    "        pred = self.head(feature)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5fffbc3-c27f-411e-b433-6d10ac0fe5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "NUM_CLIENTS = 10\n",
    "test_path = './clients/test'\n",
    "\n",
    "merged_dfs = []\n",
    "\n",
    "for mod in ['audio', 'image']:\n",
    "    dfs = []\n",
    "    for cid in range(NUM_CLIENTS):\n",
    "        csv_path = os.path.join(test_path, f'{cid}_{mod}.csv')\n",
    "        df = pd.read_csv(csv_path)\n",
    "        dfs.append(df)\n",
    "    save_path = f'./{mod}_test_total.csv'\n",
    "    merged_df = pd.concat(dfs, axis=0)\n",
    "    merged_df.to_csv(save_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "527781c1-42cf-4f0c-aca3-544c10d1bf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.mmvae.mnist.dataset import (\n",
    "    audioMNIST, imageMNIST\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "AUDIO_TEST_PATH = './audio_test_total.csv'\n",
    "IMAGE_TEST_PATH = './image_test_total.csv'\n",
    "\n",
    "# test audio clf\n",
    "audio_dataset = audioMNIST(csv_path=AUDIO_TEST_PATH)\n",
    "image_dataset = imageMNIST(csv_path=IMAGE_TEST_PATH)\n",
    "\n",
    "dl_config = {\n",
    "    'batch_size' : 128,\n",
    "    'shuffle' : False\n",
    "}\n",
    "audio_dl = DataLoader(audio_dataset, **dl_config)\n",
    "image_dl = DataLoader(image_dataset, **dl_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c55cdd2-402e-4376-a2b6-6d04248b721d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio linear prob res: 0.47\n"
     ]
    }
   ],
   "source": [
    "from utils.train import val\n",
    "if MODAL == 'audio':\n",
    "    dataloader = audio_dl\n",
    "else:\n",
    "    dataloader = image_dl\n",
    "\n",
    "clf = Classifier(\n",
    "    backbone,\n",
    "    head\n",
    ").to(DEVICE)\n",
    "    \n",
    "    \n",
    "# test audio clf\n",
    "acc = val(\n",
    "    clf,\n",
    "    dataloader,\n",
    "    DEVICE\n",
    ")\n",
    "print('{} linear prob res: {:.2f}'.format(MODAL, acc.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629a9a0a-1d91-47ac-9524-405458ec21a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
