{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7f08349-0ef3-4a08-888a-671524a47aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from fed.utils.sampling import uniform_draw_subset\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "for i in range(10):\n",
    "    file_path = f'./clients/train/{i}.csv'\n",
    "    save_path = f'./clients/probe/{i}.csv'\n",
    "    uniform_draw_subset(file_path, 0.8, save_path)\n",
    "    \n",
    "# split probe & test file into each modality\n",
    "for state in ['probe', 'test']:\n",
    "    for i in range(10):\n",
    "        file_path = f'./clients/{state}/{i}.csv'\n",
    "        df = pd.read_csv(file_path, header=None)\n",
    "        audio_df = df.iloc[:,0].to_csv(f'./clients/{state}/{i}_audio.csv', index=False, header=['audio']) \n",
    "        image_df = df.iloc[:,1].to_csv(f'./clients/{state}/{i}_image.csv', index=False, header=['image']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa4eaba3-4e74-4686-a490-79803a3d8c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from experiments.mmvae.mnist.dataset import imageMNIST, audioMNIST\n",
    "\n",
    "NUM_CLIENT = 10\n",
    "probe_loaders: List[List[DataLoader]] = []\n",
    "test_loaders: List[List[DataLoader]] = []\n",
    "\n",
    "dataloader_config = {\n",
    "    'batch_size' : 32,\n",
    "    'shuffle' : True\n",
    "}\n",
    "\n",
    "# construct probe & test audio and image loader for each client \n",
    "for state in ['probe', 'test']:\n",
    "    for cid in range(NUM_CLIENT):\n",
    "        client_loader = []\n",
    "        for mod in ['audio', 'image']:\n",
    "            file_path = f'./clients/{state}/{cid}_{mod}.csv'\n",
    "            if mod == 'audio':\n",
    "                dataset = audioMNIST(\n",
    "                    csv_path=file_path \n",
    "                )\n",
    "            else:\n",
    "                dataset = imageMNIST(\n",
    "                    csv_path=file_path\n",
    "                )\n",
    "            dataloader = DataLoader(dataset, **dataloader_config)\n",
    "            client_loader.append(dataloader)\n",
    "        if state == 'probe':\n",
    "            probe_loaders.append(client_loader)\n",
    "        else: # state == 'test'\n",
    "            test_loaders.append(client_loader)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8ab03369-9f85-4cf1-b913-6b57243d94dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from reconstruct.mmvae import DecoupledMMVAE\n",
    "from experiments.mmvae.mnist.model import _make_mlp\n",
    "from experiments.mmvae.mnist.model import (\n",
    "    get_mnist_audio_encoder,\n",
    "    get_mnist_image_encoder\n",
    ")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# load model\n",
    "ckp_path = './iid_baseline.pt'\n",
    "ckp = torch.load(ckp_path)\n",
    "\n",
    "audio_extractor = get_mnist_audio_encoder().to(device)\n",
    "image_extractor = get_mnist_image_encoder().to(device)\n",
    "\n",
    "def make_mlp(inplanes, hidden_dim, out_dim, use_bn=False):\n",
    "    if use_bn:\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(inplanes, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(inplanes, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "    \n",
    "\n",
    "mmvae_encoder = nn.ModuleDict({\n",
    "        'audio' : make_mlp(64, 256, 128, use_bn=True),\n",
    "        'image': make_mlp(64, 256, 128, use_bn=True)\n",
    "    })\n",
    "mmvae_decoder = model = nn.ModuleDict({\n",
    "        'audio' : make_mlp(64, 256, 64, use_bn=True),\n",
    "        'image':  make_mlp(64, 256, 64, use_bn=True)\n",
    "    })\n",
    "score_fns = nn.ModuleDict({\n",
    "        'audio' : nn.MSELoss(),\n",
    "        'image' : nn.MSELoss()\n",
    "    })\n",
    "mmvae = DecoupledMMVAE(\n",
    "    encoders=mmvae_encoder,\n",
    "    decoders=mmvae_decoder,\n",
    "    latent_dim=64,\n",
    "    score_fns=score_fns,\n",
    "    device=device\n",
    ")\n",
    "# mmvae.load_state_dict(ckp['mmvae'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7a8bc49e-260c-40df-982e-056994d27efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dl, image_dl = test_loaders[0]\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "audio_dl_iter = iter(audio_dl)\n",
    "image_dl_iter = iter(image_dl)\n",
    "batch_audio_1, audio_y_1 = next(audio_dl_iter)\n",
    "batch_audio_2, audio_y_2 = next(audio_dl_iter)\n",
    "\n",
    "batch_image_1, image_y_1 = next(image_dl_iter)\n",
    "batch_image_2, image_y_2 = next(image_dl_iter)\n",
    "\n",
    "# load models\n",
    "audio_extractor.load_state_dict(ckp['audio'])\n",
    "image_extractor.load_state_dict(ckp['image'])\n",
    "\n",
    "with torch.no_grad():\n",
    "    audio_extractor.eval()\n",
    "    image_extractor.eval()\n",
    "    \n",
    "    audio_feature_1 = audio_extractor(batch_audio_1.to(device))\n",
    "    audio_feature_2 = audio_extractor(batch_audio_2.to(device))\n",
    "    \n",
    "    image_feature_1 = image_extractor(batch_image_1.to(device))\n",
    "    image_feature_2 = image_extractor(batch_image_2.to(device))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d7c45b4d-e9fb-4333-8dc2-440fa4b50719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 9, 1, 8, 9, 9, 1, 2, 1, 9, 7, 1, 1, 7, 4, 1, 1, 7, 1, 7, 7, 3, 3, 9,\n",
       "        2, 1, 1, 0, 7, 1, 3, 2])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f3653da9-c0ad-472c-8e00-cee95316adb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 7, 0, 1, 8, 3, 7, 1, 9, 1, 0, 1, 7, 1, 7, 8, 3, 1, 4, 0, 1, 3, 3,\n",
       "        1, 7, 3, 3, 2, 9, 1, 9])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_y_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a696d84f-8221-485d-876a-a73957a6c9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 9, 9, 1, 3, 1, 8, 7, 8, 5, 1, 9, 1, 1, 8, 9, 2, 1, 9, 7, 2, 2, 1, 7,\n",
       "        7, 9, 1, 9, 1, 4, 9, 9])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "73a1bc36-32e0-4cb2-849f-15fb6c3b5cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8, 23, 22, 24, 10,  1,  9, 16,  1, 12,  4,  7,  9,  5, 15, 11,  9,  9,\n",
      "         7, 10, 23,  5, 11, 17,  8, 24, 23,  2, 15,  9,  9, 16],\n",
      "       device='cuda:0')\n",
      "key: 2 -> query: 8\n",
      "key: 7 -> query: 7\n",
      "key: 1 -> query: 1\n",
      "key: 7 -> query: 7\n",
      "key: 1 -> query: 1\n",
      "key: 9 -> query: 9\n",
      "key: 2 -> query: 5\n",
      "key: 2 -> query: 2\n",
      "key: 9 -> query: 9\n",
      "key: 1 -> query: 1\n",
      "key: 3 -> query: 3\n",
      "key: 2 -> query: 7\n",
      "key: 3 -> query: 5\n",
      "key: 1 -> query: 1\n",
      "key: 9 -> query: 9\n",
      "key: 9 -> query: 9\n",
      "key: 3 -> query: 5\n",
      "key: 2 -> query: 5\n",
      "key: 7 -> query: 7\n",
      "key: 1 -> query: 1\n",
      "key: 7 -> query: 7\n",
      "key: 1 -> query: 1\n",
      "key: 7 -> query: 9\n",
      "key: 1 -> query: 1\n",
      "key: 2 -> query: 8\n",
      "key: 7 -> query: 7\n",
      "key: 7 -> query: 7\n",
      "key: 2 -> query: 9\n",
      "key: 8 -> query: 9\n",
      "key: 2 -> query: 5\n",
      "key: 2 -> query: 5\n",
      "key: 2 -> query: 2\n"
     ]
    }
   ],
   "source": [
    "audio = [0, 1, 2, 3]\n",
    "image = [0, 1, 2, 3]\n",
    "\n",
    "def pair_retrieval(x: torch.Tensor, y: torch.Tensor, normalize=True):\n",
    "    # if normalize set to True, do normalize before compute dot product\n",
    "    y_normalizer = torch.linalg.vector_norm(y, dim=-1, keepdim=True)\n",
    "    normed_y = y / y_normalizer\n",
    "    \n",
    "    score_matrix = x @ normed_y.T\n",
    "    query_res = torch.argmax(score_matrix, dim=-1)\n",
    "    return query_res\n",
    "    \n",
    "keys = image_feature_2\n",
    "pool = image_feature_1\n",
    "\n",
    "keys_label = image_y_2\n",
    "pool_label = image_y_1\n",
    "\n",
    "query_res = pair_retrieval(keys, pool)\n",
    "print(query_res)\n",
    "for key, queried_idx in zip(keys_label, query_res):\n",
    "    print(f'key: {key} -> query: {pool_label[queried_idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d569652-f5b6-405d-a5c4-9d3ae781536f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0578,  0.0135,  0.0449,  0.0619, -0.0552], device='cuda:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_feature[0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee78ec26-316d-4afa-8f64-384a62579e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0118, -0.0213, -0.0110, -0.0219, -0.0025], device='cuda:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_feature[0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecc6841-a5c0-4a46-a464-bf3f34d830ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear probe on client's local dataset using server's model\n",
    "# then record performance on test set\n",
    "from utils.train import linear_prob, val\n",
    "from experiments.mmvae.mnist.model import _make_mlp\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "res = []\n",
    "NUM_CLIENTS = 10\n",
    "\n",
    "def prepare_models(audio_model, image_model, ckp):\n",
    "    audio_model.load_state_dict(ckp['audio'])\n",
    "    image_model.load_state_dict(ckp['image'])\n",
    "    return audio_model, image_model\n",
    "\n",
    "for cid in range(NUM_CLIENTS):\n",
    "    # probe\n",
    "    probe_dls = probe_loaders[cid]\n",
    "    test_dls = test_loaders[cid]\n",
    "    \n",
    "    local_audio_model, local_image_model = prepare_models(\n",
    "        audio_extractor, image_extractor, ckp\n",
    "    )\n",
    "\n",
    "    \n",
    "    to_probe = [local_audio_model, local_image_model]\n",
    "    client_eva_res = []\n",
    "    idx2modname = ['audio', 'image']\n",
    "    for idx, model in enumerate(to_probe):\n",
    "        model.eval()\n",
    "        head = _make_mlp(64, 256, 10).to(device)\n",
    "        optimizer = optim.Adam(head.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "        linear_prob(\n",
    "            model, head, \n",
    "            probe_dls[idx],\n",
    "            optimizer, criterion,\n",
    "            device,\n",
    "            n_epoch=30,\n",
    "            use_tqdm=False\n",
    "        )\n",
    "        \n",
    "        # eval\n",
    "        accuracy = val(model, test_dls[idx], device)\n",
    "        print(f'client {cid} mod {idx2modname[idx]}: {100 * accuracy:.2f}%')\n",
    "        client_eva_res.append(accuracy)\n",
    "    res.append(client_eva_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b78781-ab5b-45eb-b909-1a1fa2f4d7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
