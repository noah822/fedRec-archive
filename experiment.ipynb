{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4fd6251-ee92-4d12-aca7-785247e30d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reconstruct.mmvae import DecoupledMMVAE\n",
    "from experiments.mmvae.mnist.model import _make_mlp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "def make_mlp(inplanes, hidden_dim, out_dim, use_bn=False):\n",
    "    if use_bn:\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(inplanes, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(inplanes, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "    \n",
    "\n",
    "encoders = nn.ModuleDict({\n",
    "        'audio' : make_mlp(64, 32, 32, use_bn=True),\n",
    "        'image': make_mlp(64, 32, 32, use_bn=True)\n",
    "    }).to(device)\n",
    "# encoders.eval()\n",
    "\n",
    "decoders = nn.ModuleDict({\n",
    "        'audio' : make_mlp(16, 32, 64, use_bn=True),\n",
    "        'image': make_mlp(16, 32, 64, use_bn=True)\n",
    "    }).to(device)\n",
    "# decoders.eval()\n",
    "\n",
    "score_fns = nn.ModuleDict({\n",
    "        'audio' : nn.MSELoss(),\n",
    "        'image' : nn.MSELoss()\n",
    "    }).to(device)\n",
    "\n",
    "mmvae = DecoupledMMVAE(\n",
    "    encoders,\n",
    "    decoders,\n",
    "    16,\n",
    "    score_fns,\n",
    "    device\n",
    ").to(device)\n",
    "    \n",
    "# load pretrained mmvae from last iter\n",
    "ckp_path = './ckp/components/13/mmvae_moco_test_otherway_train.pt'\n",
    "ckp = torch.load(ckp_path)\n",
    "\n",
    "# mmvae.load_state_dict(ckp['mmvae']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f095df4-8577-4729-b6c5-ef1debcf790c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try other mmvae method\n",
    "from experiments.mmvae.mnist.dataset import mmMNIST\n",
    "\n",
    "\n",
    "public_dataset_path = '/root/autodl-tmp/csv/mmMNIST_server.csv'\n",
    "public_dataset = mmMNIST(public_dataset_path)\n",
    "len(public_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e32e0a66-a33a-4e9d-bb02-f6e116757235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fed.utils.pipeline import PairedFeatureBank, MMVAETrainer\n",
    "from experiments.mmvae.mnist.model import (\n",
    "    get_mnist_audio_encoder,\n",
    "    get_mnist_image_encoder\n",
    ")\n",
    "\n",
    "audio_backbone = get_mnist_audio_encoder().to(device)\n",
    "audio_backbone.load_state_dict(ckp['audio'])\n",
    "\n",
    "image_backbone = get_mnist_image_encoder().to(device)\n",
    "image_backbone.load_state_dict(ckp['image'])\n",
    "\n",
    "embed_dataset = PairedFeatureBank(\n",
    "        public_dataset,\n",
    "        (audio_backbone, image_backbone, ),\n",
    "        device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "328d6da2-9fa4-488e-8c41-2ae6ef24b990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c392e25e308148cba6f5c1cd99138b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93723da91ecf48c18413853b3b81f9ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m optim_config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m1e-2\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m1e-5\u001b[39m\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m      9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m MMVAETrainer(\n\u001b[1;32m     10\u001b[0m     mmvae,\n\u001b[1;32m     11\u001b[0m     embed_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     cross_loss_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m fitted_mmvae \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mexport_mmvae()\n",
      "File \u001b[0;32m~/fedRec/fed/utils/pipeline.py:145\u001b[0m, in \u001b[0;36mMMVAETrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m           prompt[k] \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    143\u001b[0m      \u001b[38;5;28;01mreturn\u001b[39;00m nelbo, prompt\n\u001b[0;32m--> 145\u001b[0m \u001b[43mvanilla_trainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmmvae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m     \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m     \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m     \u001b[49m\u001b[43m_unpack_and_forward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdo_autoencode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcustom_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fedRec/utils/train.py:226\u001b[0m, in \u001b[0;36mvanilla_trainer\u001b[0;34m(model, dataloader, optimizer, criterion, n_epoch, device, unpack_and_forward, do_autoencode, scheduler, post_bp_operation, use_tqdm, custom_prompt, use_tensorboard, tb_write_list, tensorboard_path, do_val, val_freq, valloader, save_path)\u001b[0m\n\u001b[1;32m    224\u001b[0m         prompt \u001b[38;5;241m=\u001b[39m OrderedDict({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m : loss\u001b[38;5;241m.\u001b[39mitem()})\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 226\u001b[0m         loss, prompt \u001b[38;5;241m=\u001b[39m \u001b[43munpack_and_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     x, label \u001b[38;5;241m=\u001b[39m inputs\n",
      "File \u001b[0;32m~/fedRec/fed/utils/pipeline.py:141\u001b[0m, in \u001b[0;36mMMVAETrainer.train.<locals>._unpack_and_forward\u001b[0;34m(model, inputs)\u001b[0m\n\u001b[1;32m    134\u001b[0m prompt \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    135\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnelbo\u001b[39m\u001b[38;5;124m'\u001b[39m : nelbo\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    136\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkl\u001b[39m\u001b[38;5;124m'\u001b[39m : kl\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    137\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrec\u001b[39m\u001b[38;5;124m'\u001b[39m : rec\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    138\u001b[0m }\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m verbose_output\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 141\u001b[0m      prompt[k] \u001b[38;5;241m=\u001b[39m \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m()\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nelbo, prompt\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "dataloader_config = {\n",
    "            'batch_size' : 32,\n",
    "            'shuffle' : True,\n",
    "        }\n",
    "optim_config = {\n",
    "    'lr' : 1e-2,\n",
    "    'weight_decay' : 1e-5\n",
    "}\n",
    "trainer = MMVAETrainer(\n",
    "    mmvae,\n",
    "    embed_dataset,\n",
    "    dataloader_config,\n",
    "    optim_config,\n",
    "    cross_loss_only=False\n",
    ")\n",
    "trainer.train()\n",
    "fitted_mmvae = trainer.export_mmvae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a65eb0e-b359-4a45-9497-856513582cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mmvae.load_state_dict(fitted_mmvae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf531d51-ec25-48f3-b8bb-4bfc0a9ec644",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmvae.load_state_dict(ckp['mmvae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09689979-a83d-4a38-a687-b5240cab7053",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmvae.eval()\n",
    "num_sample = 1000\n",
    "res = mmvae.generate(num_sample=num_sample)\n",
    "res['audio'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb13bde4-7905-445c-9b86-17cf1e7717ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_numpy\u001b[39m(t):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m----> 5\u001b[0m gen_audio_embed \u001b[38;5;241m=\u001b[39m _to_numpy(\u001b[43mres\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      6\u001b[0m gen_image_embed \u001b[38;5;241m=\u001b[39m _to_numpy(res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      9\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_sample)] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_sample)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'res' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def _to_numpy(t):\n",
    "    return t.contiguous().detach().cpu().numpy()\n",
    "\n",
    "gen_audio_embed = _to_numpy(res['audio'])\n",
    "gen_image_embed = _to_numpy(res['image'])\n",
    "\n",
    "\n",
    "labels = [0 for _ in range(num_sample)] + [1 for _ in range(num_sample)]\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b536a90-0435-425f-ae42-b8e4ea95ec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "feature_bank = np.concatenate([gen_audio_embed, gen_image_embed], axis=0)\n",
    "embeds = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(feature_bank)\n",
    "# colors = cm.rainbow(np.linspace(0, 1))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "names = ['gen_audio', 'gen_image']\n",
    "for idx, name in enumerate(names):\n",
    "    indices = np.where(labels == idx)\n",
    "    plt.scatter(embeds[indices, 0], embeds[indices, 1], label=f'{name}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f61ad25-b9f6-43d9-87b7-f1a4c42d1fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some real image/audio data\n",
    "from experiments.mmvae.mnist.dataset import (\n",
    "    audioMNIST, imageMNIST\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "AUDIO_TEST_PATH = './audio_test_total.csv'\n",
    "IMAGE_TEST_PATH = './image_test_total.csv'\n",
    "\n",
    "# test audio clf\n",
    "audio_dataset = audioMNIST(csv_path=AUDIO_TEST_PATH)\n",
    "image_dataset = imageMNIST(csv_path=IMAGE_TEST_PATH)\n",
    "\n",
    "dl_config = {\n",
    "    'batch_size' : 1000,\n",
    "    'shuffle' : False\n",
    "}\n",
    "audio_dl = DataLoader(audio_dataset, **dl_config)\n",
    "image_dl = DataLoader(image_dataset, **dl_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614737c2-6d4e-413b-bdac-25942484b497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.mmvae.mnist.model import (\n",
    "    get_mnist_audio_encoder,\n",
    "    get_mnist_image_encoder\n",
    ")\n",
    "\n",
    "audio_backbone = get_mnist_audio_encoder().to(device)\n",
    "audio_backbone.load_state_dict(ckp['audio'])\n",
    "\n",
    "image_backbone = get_mnist_image_encoder().to(device)\n",
    "image_backbone.load_state_dict(ckp['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6839dc90-4bb1-4c88-b557-cb58e111f490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute embeds\n",
    "real_audio_embed = []\n",
    "real_image_embed = []\n",
    "audio_batch, _ = next(iter(audio_dl))\n",
    "image_batch, _ = next(iter(image_dl))\n",
    "\n",
    "real_audio_embed = _to_numpy(audio_backbone(audio_batch.to(device)))\n",
    "real_image_embed = _to_numpy(image_backbone(image_batch.to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94fa576-2d8a-450a-b58c-4ae1af8314ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_bank = np.concatenate([real_audio_embed, real_image_embed], axis=0)\n",
    "\n",
    "labels = []\n",
    "for i in range(4):\n",
    "    labels += [i for _ in range(num_sample)]\n",
    "labels = np.array(labels)\n",
    "    \n",
    "embeds = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(feature_bank)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "names = ['real_audio', 'real_image']\n",
    "for idx, name in enumerate(names):\n",
    "    indices = np.where(labels == idx)\n",
    "    plt.scatter(embeds[indices, 0], embeds[indices, 1], label=f'{name}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77d1e93-33ca-46c9-99ee-7ef678326358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try conditional generation\n",
    "image_cond_audio = []\n",
    "wrapped_inputs = {'audio' : torch.from_numpy(real_audio_embed).to(device)}\n",
    "cond_gen = mmvae.reconstruct(wrapped_inputs)\n",
    "\n",
    "image_cond_audio = _to_numpy(cond_gen['image'])\n",
    "audio_cond_audio = _to_numpy(cond_gen['audio'])\n",
    "\n",
    "feature_bank = np.concatenate([\n",
    "    real_audio_embed,\n",
    "    real_image_embed,\n",
    "    audio_cond_audio,\n",
    "    image_cond_audio,\n",
    "    np.mean(real_audio_embed, axis=0, keepdims=True),\n",
    "    np.mean(real_image_embed, axis=0, keepdims=True)\n",
    "\n",
    "],\n",
    "    axis=0)\n",
    "\n",
    "labels = []\n",
    "cate_num = 4\n",
    "for i in range(cate_num):\n",
    "    labels += [i for _ in range(num_sample)]\n",
    "labels = np.array(labels + [cate_num, cate_num+1])\n",
    "    \n",
    "embeds = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(feature_bank)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "names = ['real_audio', 'real_image', 'rec_audio', 'rec_image', 'c_a', 'c_i']\n",
    "for idx, name in enumerate(names):\n",
    "    indices = np.where(labels == idx)\n",
    "    plt.scatter(embeds[indices, 0], embeds[indices, 1], label=f'{name}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4ccc05-bd43-4d0b-abc4-8a39ebf994da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try conditional generation\n",
    "wrapped_inputs = {'image' : torch.from_numpy(real_image_embed).to(device)}\n",
    "cond_gen = mmvae.reconstruct(wrapped_inputs)\n",
    "\n",
    "image_cond_image = _to_numpy(cond_gen['image'])\n",
    "audio_cond_image = _to_numpy(cond_gen['audio'])\n",
    "\n",
    "feature_bank = np.concatenate([real_audio_embed, real_image_embed, audio_cond_image, image_cond_audio], axis=0)\n",
    "\n",
    "labels = []\n",
    "for i in range(4):\n",
    "    labels += [i for _ in range(num_sample)]\n",
    "labels = np.array(labels)\n",
    "    \n",
    "embeds = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(feature_bank)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "names = ['real_audio', 'real_image', 'rec_audio', 'rec_image']\n",
    "for idx, name in enumerate(names):\n",
    "    indices = np.where(labels == idx)\n",
    "    plt.scatter(embeds[indices, 0], embeds[indices, 1], label=f'{name}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5219884c-7173-4c14-83b1-38283a760066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
